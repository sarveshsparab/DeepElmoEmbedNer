{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> \n",
       "    table {display: block;} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> \n",
    "    table {display: block;} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep contextualized word representations\n",
    "## Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer\n",
    "### Code\n",
    "- https://github.com/blackbbc/NER\n",
    "- https://github.com/sarveshsparab/DeepElmoEmbedNer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding paths to sys paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supressing warning level messages in output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating an object of the NER parent class implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepElmoEmbedNer import DeepElmoEmbedNer\n",
    "\n",
    "deen = DeepElmoEmbedNer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the input files for the model\n",
    "### 3 files required\n",
    "- train : For the model to train\n",
    "- dev : For the model to validate the training\n",
    "- test : To evaluate the performance to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = dict()\n",
    "file_dict['train'] = '../data/sample/ner_test_input.txt'\n",
    "file_dict['test'] = '../data/sample/ner_test_input.txt'\n",
    "file_dict['dev'] = '../data/sample/ner_test_input.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from the dataset\n",
    "- Description\n",
    "    * Reads a dataset in preparation for train or test. Returns data in proper format for train or test.\n",
    "- Returns\n",
    "    * A dictionary of file_dict keys as keys and values as lists of lines, where in each line is further tokenized on the column delimiter and extracted as a list\n",
    "- Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>Name</th>\n",
    "            <th>Default</th>\n",
    "            <th>Purpose</th>\n",
    "            <th>Required</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=\"2\">Standard</td>\n",
    "            <td>file_dict</td>\n",
    "            <td>-</td>\n",
    "            <td>A dictionary with input file locations</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>dataset_name</td>\n",
    "            <td>CoNLL03</td>\n",
    "            <td>Name of the dataset required for calling appropriate utils, converters</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>args</td>\n",
    "            <td>N/A</td>\n",
    "            <td>None</td>\n",
    "            <td>N/A</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=\"2\">kwargs</td>\n",
    "            <td>fileHasHeaders</td>\n",
    "            <td>True</td>\n",
    "            <td>Flag to check if input file has headers</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>columnDelimiter</td>\n",
    "            <td>`space`</td>\n",
    "            <td>Delimiter in the data input</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deen.read_dataset(file_dict, \"CoNLL2003\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the ground truth data\n",
    "- Description\n",
    "    * Converts test data into common format for evaluation \\[i.e. same format as predict()\\] \n",
    "    * This added step/layer of abstraction is required due to the refactoring of read_dataset_train() and read_dataset_test() back to the single method of read_dataset() along with the requirement on the format of the output of predict() and therefore the input format requirement of evaluate()\n",
    "- Returns\n",
    "    * \\[tuple,...\\], i.e. list of tuples. \\[SAME format as output of predict()\\]\n",
    "    * Each tuple is (start index, span, mention text, mention type)\n",
    "    * Where:\n",
    "         - start index: int, the index of the first character of the mention span. None if not applicable.\n",
    "         - span: int, the length of the mention. None if not applicable.\n",
    "         - mention text: str, the actual text that was identified as a named entity. Required.\n",
    "         - mention type: str, the entity/mention type. None if not applicable.\n",
    "- Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>Name</th>\n",
    "            <th>Default</th>\n",
    "            <th>Purpose</th>\n",
    "            <th>Required</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Standard</td>\n",
    "            <td>data</td>\n",
    "            <td>-</td>\n",
    "            <td>data in proper format for train or test. [i.e. format of output from read_dataset]</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>args</td>\n",
    "            <td>N/A</td>\n",
    "            <td>None</td>\n",
    "            <td>N/A</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=\"4\">kwargs</td>\n",
    "            <td>wordPosition</td>\n",
    "            <td>0</td>\n",
    "            <td>Column number with the mention word</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>tagPosition</td>\n",
    "            <td>3</td>\n",
    "            <td>Column number with the entity tag</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>writeGroundTruthToFile</td>\n",
    "            <td>True</td>\n",
    "            <td>Flag to enable writing ground truths to a file</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>groundTruthPath</td>\n",
    "            <td>../results/groundTruths.txt</td>\n",
    "            <td>Location to save the ground truths file</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth = deen.convert_ground_truth(data, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "- Description\n",
    "    * Trains he model on the parsed data\n",
    "    * Calls the internal save_model method to save the trained model for predictions\n",
    "- Returns\n",
    "    * Not Applicable\n",
    "- Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>Name</th>\n",
    "            <th>Default</th>\n",
    "            <th>Purpose</th>\n",
    "            <th>Required</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Standard</td>\n",
    "            <td>data</td>\n",
    "            <td>-</td>\n",
    "            <td>Parsed input data in the format returned by read_dataset method</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>args</td>\n",
    "            <td>N/A</td>\n",
    "            <td>None</td>\n",
    "            <td>N/A</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=\"22\">kwargs</td>\n",
    "            <td>parsedDumpPath</td>\n",
    "            <td>../dev/parsedDataDump.pkl</td>\n",
    "            <td>Location of the parsed input data-files in the pickled format</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>vocabPath</td>\n",
    "            <td>../dev/vocab.txt</td>\n",
    "            <td>Location of the parsed vocab</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>elmoOptionsFile</td>\n",
    "            <td>../resources/elmo/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json</td>\n",
    "            <td>ELMo model options parameters file</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>elmoWeightFile</td>\n",
    "            <td>../resources/elmo/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5</td>\n",
    "            <td>ELMo model weights file</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>wordEmbeddingSize</td>\n",
    "            <td>50</td>\n",
    "            <td>Set the ELMo word embedding size for the model</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>charEmbeddingSize</td>\n",
    "            <td>16</td>\n",
    "            <td>Set the ELMo character embedding size for the model</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>LSTMStateSize</td>\n",
    "            <td>200</td>\n",
    "            <td>State size of the Multi-LSTM layers</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>filterNum</td>\n",
    "            <td>128</td>\n",
    "            <td>Filter area size</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>filterSize</td>\n",
    "            <td>3</td>\n",
    "            <td>Number of filters in the model</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>learningRate</td>\n",
    "            <td>0.015</td>\n",
    "            <td>Model learning rate</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>dropoutRate</td>\n",
    "            <td>0.5</td>\n",
    "            <td>Model dropout rate</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>epochWidth</td>\n",
    "            <td>16</td>\n",
    "            <td>Batch size within each epoch</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>maxEpoch</td>\n",
    "            <td>100</td>\n",
    "            <td>Number of epoch to run for training</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>checkpointPath</td>\n",
    "            <td>../results/checkpoints</td>\n",
    "            <td>Location to save intermediate checkpoints</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>bestCheckpointPath</td>\n",
    "            <td>../results/checkpoints/best</td>\n",
    "            <td>Location to save the best F1 returning</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>trainWordsPath</td>\n",
    "            <td>../dev/train.word.vocab</td>\n",
    "            <td>Location to save the intermediate vocabulary words from the training set</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>trainCharPath</td>\n",
    "            <td>../dev/train.char.vocab</td>\n",
    "            <td>Location to save the intermediate vocabulary characters from the training set</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>gloveEmbedPath</td>\n",
    "            <td>../resources/glove/glove.6B.50d.txt</td>\n",
    "            <td>Location fo the glove embedding file</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>fetchPredictData</td>\n",
    "            <td>False</td>\n",
    "            <td>Flag to toggle behaviour of the internal data_converter method</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>maxWordLength</td>\n",
    "            <td>30</td>\n",
    "            <td>Set maximal word length for the model</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>wordPosition</td>\n",
    "            <td>0</td>\n",
    "            <td>Column number with the mention word</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>tagPosition</td>\n",
    "            <td>3</td>\n",
    "            <td>Column number with the entity tag</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, sess, saver = deen.train(data, None, maxEpoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating predictions\n",
    "- Description\n",
    "    * Parses and converts the input sentence provided in a file for predicting the NER tags\n",
    "    * Calls the internal load_model method to load the trained model for predictions\n",
    "- Returns\n",
    "    * \\[tuple,...\\], i.e. list of tuples.\n",
    "    * Each tuple is (start index, span, mention text, mention type)\n",
    "    * Where:\n",
    "         - start index: int, the index of the first character of the mention span. None if not applicable.\n",
    "         - span: int, the length of the mention. None if not applicable.\n",
    "         - mention text: str, the actual text that was identified as a named entity. Required.\n",
    "         - mention type: str, the entity/mention type. None if not applicable.\n",
    "\n",
    "         `NOTE: len(predictions) should equal len(data) AND the ordering should not change [important for evaluation. See note in evaluate() about parallel arrays.]`\n",
    "- Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>Name</th>\n",
    "            <th>Default</th>\n",
    "            <th>Purpose</th>\n",
    "            <th>Required</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Standard</td>\n",
    "            <td>data</td>\n",
    "            <td>-</td>\n",
    "            <td>The file location with the input text in the common format for prediction</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>args</td>\n",
    "            <td>N/A</td>\n",
    "            <td>None</td>\n",
    "            <td>N/A</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=\"11\">kwargs</td>\n",
    "            <td>model</td>\n",
    "            <td>N/A</td>\n",
    "            <td>ElmoModel instance to hold the loaded model into</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>sess</td>\n",
    "            <td>N/A</td>\n",
    "            <td>Tensorflow.Session instance used to maintain the same session used to train</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>saver</td>\n",
    "            <td>N/A</td>\n",
    "            <td>Tensorflow.train.saver instance used to load the trained model</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>trainedData</td>\n",
    "            <td>N/A</td>\n",
    "            <td>Parsed trained data</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>fileHasHeaders</td>\n",
    "            <td>True</td>\n",
    "            <td>Flag to check if input file has headers</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>parsedDumpPath</td>\n",
    "            <td>../dev/parsedDataDump.pkl</td>\n",
    "            <td>Location of the parsed input data-files in the pickled format</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>bestCheckpointPath</td>\n",
    "            <td>../results/checkpoints/best</td>\n",
    "            <td>Location to save the best F1 returning</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>epochWidth</td>\n",
    "            <td>16</td>\n",
    "            <td>Batch size within each epoch</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>writePredsToFile</td>\n",
    "            <td>True</td>\n",
    "            <td>Flag to enable writing predictions to file</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>predsPath</td>\n",
    "            <td>../results/predictions.txt</td>\n",
    "            <td>Location where to write predictions into</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>writeInputToFile</td>\n",
    "            <td>False</td>\n",
    "            <td>Flag to toggle behaviour of the internal data_converter method</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = deen.predict('../data/sample/ner_test_input.txt', None, writeInputToFile=False, model=model, sess=sess, saver=saver, trainedData=data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model\n",
    "- Description\n",
    "    * Calculates evaluation metrics on chosen benchmark dataset\n",
    "        - Precision\n",
    "        - Recall\n",
    "        - F1 Score\n",
    "- Returns\n",
    "    * Tuple with metrics (p,r,f1). Each element is float.\n",
    "- Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Type</th>\n",
    "            <th>Name</th>\n",
    "            <th>Default</th>\n",
    "            <th>Purpose</th>\n",
    "            <th>Required</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td rowspan=\"2\">Standard</td>\n",
    "            <td>predictions</td>\n",
    "            <td>N/A</td>\n",
    "            <td>List of predicted labels</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>groundTruths</td>\n",
    "            <td>N/A</td>\n",
    "            <td>List of ground truth labels</td>\n",
    "            <td>✔</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>args</td>\n",
    "            <td>N/A</td>\n",
    "            <td>None</td>\n",
    "            <td>N/A</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=\"2\">kwargs</td>\n",
    "            <td>predsPath</td>\n",
    "            <td>../results/predictions.txt</td>\n",
    "            <td>Location from where to read predictions from</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>groundTruthPath</td>\n",
    "            <td>../results/groundTruths.txt</td>\n",
    "            <td>Location from where to read ground truths from</td>\n",
    "            <td>✖</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deen.evaluate([col[3] for col in predictions], [col[3] for col in groundTruth], None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
